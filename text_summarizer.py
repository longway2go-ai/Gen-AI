# -*- coding: utf-8 -*-
"""Text Summarizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s6v1y8zqRVZmH-Nc8Mdu6YVmfqA5MQpG
"""

!pip install transformers[sentencepiece] datasets rouge_score sacrebleu py7zr -q

!pip install transformers accelerate

from transformers import pipeline, set_seed
from datasets import load_dataset,load_from_disk
import matplotlib.pyplot as plt
import nltk
from nltk.tokenize import sent_tokenize
import pandas as pd
from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
from tqdm import tqdm
import torch

nltk.download('punkt')

device= 'cuda' if torch.cuda.is_available() else 'cpu'
device

model1="google/pegasus-cnn_dailymail"
tokenizer1= AutoTokenizer.from_pretrained(model1)

model= AutoModelForSeq2SeqLM.from_pretrained(model1).to(device)

!pip install arxiv

import arxiv

prompt = "generative ai or agentic ai or machine learning or deep learning"
search=arxiv.Search(query=prompt,max_results=10,sort_by=arxiv.SortCriterion.Relevance)

papers=[]
for result in search.results():
  papers.append({
      'published': result.published,
      'title': result.title,
      'summary': result.summary,
      'authors': result.authors,
      'url': result.pdf_url,
      'categories' : result.categories
  })

  df=pd.DataFrame(papers)

pd.set_option('display.max_colwidth',None)
df.head()

summary=df['summary'][3]

summarizer=pipeline('summarization',model=model1,tokenizer=tokenizer1)
summarization_result=summarizer(summary)

summarization_result[0]['summary_text']

!rm -rf ~/.cache/huggingface/datasets

